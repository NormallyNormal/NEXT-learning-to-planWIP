"""
Meta Self-Improving Learning (MSIL) Training Script for NEXT

This script implements the training procedure described in Section 4.3 of:
"Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees"
(Chen et al., ICLR 2020)

The key idea is to train the policy and value networks by imitating successful
planning trajectories generated by the algorithm itself, creating a self-improving
loop where better networks lead to better trajectories which further improve the networks.

Algorithm 3: Meta Self-Improving Learning
-----------------------------------------
1. Initialize dataset D_0
2. for epoch n <- 1 to N do
3.   Sample a planning problem U
4.   T <- TSA(U) with epsilon ~ Unif[0,1], mixing:
      epsilon * RRT::Expand + (1-epsilon) * NEXT::Expand
      Postprocessing with RRT*::Postprocess
5.   D_n <- D_{n-1} ∪ {(T, U)} if successful else D_{n-1}
6.   for j <- 0 to L do
7.     Sample (T_j, U_j) from D_n
8.     Reconstruct sub-optimal path {s^i}_{i=1}^m and costs
9.     Update W <- W - eta * grad_W(loss)
10.  Anneal epsilon = alpha * epsilon, alpha in (0, 1)

From Appendix E.2:
- Epsilon annealing schedule (changes every 200 problems in range 1000-2000):
  - epsilon = 1 if i < 1000
  - epsilon = 0.5 - 0.1 * floor((i-1000)/200) if 1000 <= i < 2000
  - epsilon = 0.1 otherwise
- Total: 3000 problems (2000 training, 1000 test)

Loss function (Equation 6):
    L = -sum_i log(pi(s_{i+1}|s_i)) + sum_i (V(s_i) - y_i)^2 / 2 + lambda * ||W||^2

Where y_i is the cost-to-go from state s_i to the goal.
"""

import argparse
import os
import random
from collections import deque

import numpy as np
import torch
import torch.optim as optim

from environment import MazeEnv
from model import Model
from algorithm import NEXT_plan, RRTS_plan
from utils import set_random_seed, load_model, mkdir_if_not_exist


class ExperienceReplayBuffer:
    """
    Experience replay buffer for storing successful planning trajectories.

    As described in Section 4.3: "We fix the size of dataset and update D
    in the same way as experience replay buffer"

    Each experience contains:
    - path: list of states from start to goal
    - costs_to_go: cost-to-go value for each state in the path
    - problem: the planning problem (map, init_state, goal_state)
    """

    def __init__(self, capacity=2000):
        self.buffer = deque(maxlen=capacity)

    def add(self, path, costs_to_go, problem):
        """Add a successful trajectory to the buffer."""
        self.buffer.append({
            'path': path,
            'costs_to_go': costs_to_go,
            'problem': problem
        })

    def sample(self, batch_size=1):
        """Sample experiences from the buffer."""
        if len(self.buffer) < batch_size:
            return list(self.buffer)
        return random.sample(list(self.buffer), batch_size)

    def get(self, idx):
        if idx < len(self.buffer):
            return self.buffer[idx]
        raise IndexError

    def __len__(self):
        return len(self.buffer)


def extract_path_from_tree(search_tree, env):
    """
    Extract the successful path from the search tree.

    The path is reconstructed by following parent pointers from a goal state
    back to the root (initial state). As described in Section 4.3:
    "we can reconstruct the successful path {s^i}_{i=1}^m from the search tree T"

    Args:
        search_tree: SearchTree object containing the planning result
        env: Environment object

    Returns:
        path: List of states from start to goal
        costs: List of segment costs
    """
    # Find a state in the goal region
    goal_idx = None
    for i, in_goal in enumerate(search_tree.in_goal_region):
        if in_goal and search_tree.freesp[i]:
            goal_idx = i
            break

    if goal_idx is None:
        return None, None

    # Reconstruct path by following (rewired) parent pointers
    path = []
    costs = []
    idx = goal_idx

    while idx is not None:
        path.append(search_tree.states[idx])
        parent_idx = search_tree.rewired_parents[idx]
        if parent_idx is not None:
            # Cost of segment from parent to current
            cost = env.distance(search_tree.states[parent_idx], search_tree.states[idx])
            costs.append(cost)
        idx = parent_idx

    # Reverse to get path from start to goal
    path = path[::-1]
    costs = costs[::-1]

    return path, costs


def compute_costs_to_go(costs):
    """
    Compute cost-to-go for each state in the path.

    As described in Section 4.3:
    "the value of each state s^i in the path will be the sum of cost to the goal region"
    y_i := sum_{l=i}^{m-1} c([s^l, s^{l+1}])

    Args:
        costs: List of segment costs [c_0, c_1, ..., c_{m-1}]

    Returns:
        costs_to_go: List of cost-to-go values [y_0, y_1, ..., y_{m-1}, 0]
    """
    # Convert to numpy array for easier computation and ensure float type
    costs_arr = np.array(costs, dtype=np.float32)
    total = float(np.sum(costs_arr))

    costs_to_go = []
    cumsum = 0.0
    for c in costs_arr:
        costs_to_go.append(total - cumsum)
        cumsum += float(c)
    costs_to_go.append(0.0)  # Cost-to-go at goal is 0

    return costs_to_go

gaussianNLLLoss = torch.nn.GaussianNLLLoss(reduction='sum')

def compute_loss(model, path, costs_to_go, device):
    dim = model.dim
    std = model.std

    states = torch.FloatTensor(np.array(path))
    targets_value = torch.FloatTensor(np.array(costs_to_go))

    if model.cuda:
        states = states.cuda()
        targets_value = targets_value.cuda()

    y = model.net.state_forward(states, model.pb_rep)
    pred_actions = y[:, :dim]
    pred_values = y[:, -1]

    policy_loss = torch.tensor(0.0, device=device, requires_grad=True)

    if len(path) > 1:
        actions = states[1:] - states[:-1]  # (m-1, dim)
        action_means = pred_actions[:-1]     # (m-1, dim)

        var = std ** 2

        policy_loss = gaussianNLLLoss(action_means, actions, torch.full_like(action_means, var))

    value_loss = ((pred_values - targets_value) ** 2).sum()

    total_loss = policy_loss + value_loss

    return total_loss, policy_loss.item(), value_loss.item()


def get_epsilon(problem_idx):
    """
    Epsilon schedule from Appendix E.2 of the paper.

    "The value of the annealing epsilon was set as the following:
     epsilon = 1, if i < 1000
     epsilon = 0.5 - 0.1 * floor((i-1000)/200), if 1000 <= i < 2000
     epsilon = 0.1, otherwise
    with i denoting the problem number."

    Args:
        problem_idx: Current problem index (0-based)

    Returns:
        epsilon: Probability of using RRT exploration vs NEXT guidance
    """
    if problem_idx < 1000:
        return 1.0
    elif problem_idx < 2000:
        return 0.5 - 0.1 * ((problem_idx - 1000) // 200)
    else:
        return 0.1


def run_planning(env, model, T, epsilon, use_model=True):
    """
    Run planning with epsilon-mixture of RRT and NEXT expansion.

    As described in Section 4.3:
    "we use a mixture of RRT::Expand and NEXT::Expand with probability
    epsilon and 1-epsilon, respectively"

    Args:
        env: Environment object
        model: Model object
        T: Maximum number of samples
        epsilon: Probability of RRT exploration (g_explore_eps in NEXT_plan)
        use_model: Whether to use the model (False for pure RRT when epsilon=1)

    Returns:
        search_tree: The resulting search tree
        success: Whether a path was found
    """
    if epsilon >= 1.0 or not use_model:
        # Pure RRT* (epsilon = 1 means always use RRT::Expand)
        return RRTS_plan(env=env, T=T, stop_when_success=True)
    else:
        # NEXT with epsilon-mixture
        # g_explore_eps controls probability of global (RRT-style) exploration
        return NEXT_plan(
            env=env,
            model=model,
            T=T,
            g_explore_eps=epsilon,
            stop_when_success=True
        )


def train(args):
    """
    Main training loop implementing Meta Self-Improving Learning (Algorithm 3).

    This follows Algorithm 3 exactly:
    - Process problems one at a time
    - After each problem, do L gradient updates (if buffer has enough samples)
    - Epsilon follows the schedule in Appendix E.2
    """
    # Set random seed for reproducibility
    set_random_seed(args.seed)

    # Setup device
    device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Initialize environment
    print(f"\nInitializing {args.dim}D environment...")
    env = MazeEnv(dim=args.dim)
    num_problems = min(args.num_problems, env.size)
    print(f"Total problems available: {env.size}")
    print(f"Problems to process: {num_problems}")

    # Initialize model
    model = Model(cuda=args.cuda, dim=args.dim)

    # Load pretrained model if specified
    if args.pretrained:
        print(f"Loading pretrained model from {args.pretrained}")
        load_model(model.net, args.pretrained, args.cuda)

    # Setup optimizer
    optimizer = optim.Adam(model.net.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # Initialize experience replay buffer (Algorithm 3, line 1)
    # "We fix the size of dataset and update D in the same way as experience replay buffer"
    replay_buffer = ExperienceReplayBuffer(capacity=args.buffer_size)

    # Create save directory
    mkdir_if_not_exist(args.save_dir)

    # Training statistics
    stats = {
        'problem_idx': [],
        'epsilon': [],
        'success': [],
        'path_cost': [],
        'tree_size': [],
        'policy_loss': [],
        'value_loss': [],
        'total_loss': [],
        'num_updates': []
    }

    # Tracking for periodic reporting
    recent_successes = []
    recent_path_costs = []
    total_gradient_steps = 0

    print("\n" + "=" * 70)
    print("Starting Meta Self-Improving Learning (Algorithm 3)")
    print("=" * 70)
    print(f"\nTraining schedule:")
    print(f"  - L = {args.L} gradient updates per problem (Algorithm 3, lines 6-9)")
    print(f"  - Epsilon schedule (Appendix E.2):")
    print(f"      Problems 0-999:    eps = 1.0 (pure RRT)")
    print(f"      Problems 1000-1199: eps = 0.5")
    print(f"      Problems 1200-1399: eps = 0.4")
    print(f"      Problems 1400-1599: eps = 0.3")
    print(f"      Problems 1600-1799: eps = 0.2")
    print(f"      Problems 1800-1999: eps = 0.1")
    print(f"      Problems 2000+:    eps = 0.1")
    print()

    # =========================================================================
    # Algorithm 3, line 2: for epoch n <- 1 to N do
    # =========================================================================
    for problem_idx in range(num_problems):

        # Get epsilon for this problem (Appendix E.2 schedule)
        epsilon = get_epsilon(problem_idx)

        # =====================================================================
        # Algorithm 3, line 3: Sample a planning problem U
        # =====================================================================
        problem = env.init_new_problem(problem_idx)

        # Set problem for model (precompute goal representation)
        with torch.no_grad():
            if epsilon < 1.0:
                model.set_problem(problem)

        # =====================================================================
        # Algorithm 3, line 4: T <- TSA(U) with epsilon-mixture
        # "T <- TSA(U) with epsilon ~ Unif[0,1], and
        #  epsilon * RRT::Expand + (1-epsilon) * NEXT::Expand
        #  Postprocessing with RRT*::Postprocess"
        # =====================================================================

            search_tree, success = run_planning(
                env=env,
                model=model,
                T=args.max_samples,
                epsilon=epsilon,
                use_model=(epsilon < 1.0)
            )

        tree_size = len(search_tree.states)
        path_cost = -1.0

        # =====================================================================
        # Algorithm 3, line 5: D_n <- D_{n-1} ∪ {(T, U)} if successful
        # =====================================================================
        if success:
            path, costs = extract_path_from_tree(search_tree, env)

            if path is not None and len(path) > 1:
                costs_to_go = compute_costs_to_go(costs)
                replay_buffer.add(path, costs_to_go, problem)
                path_cost = float(np.sum(costs))
                recent_path_costs.append(path_cost)

        recent_successes.append(1 if success else 0)

        # =====================================================================
        # Algorithm 3, lines 6-9: for j <- 0 to L do ... Update W
        # L gradient updates after EACH problem
        # =====================================================================
        num_updates_this_problem = 0
        avg_policy_loss = -1.0
        avg_value_loss = -1.0
        avg_total_loss = -1.0

        if len(replay_buffer) >= args.min_buffer_size:
            policy_losses = []
            value_losses = []
            total_losses = []

            for j in range(len(replay_buffer)):
                # Algorithm 3, line 7: Sample (T_j, U_j) from D_n
                experience = replay_buffer.get(j)
                optimizer.zero_grad()

                model.set_problem(experience['problem'])
                loss, policy_loss, value_loss = compute_loss(
                    model=model,
                    path=experience['path'],
                    costs_to_go=experience['costs_to_go'],
                    device=device
                )

                loss.backward()
                optimizer.step()

                policy_losses.append(policy_loss)
                value_losses.append(value_loss)
                total_losses.append(loss)
                total_gradient_steps += 1

            print(f"Loss: {np.mean(policy_losses)}")

        # =====================================================================
        # Algorithm 3, line 10: Anneal epsilon = alpha * epsilon
        # Note: We use the deterministic schedule from Appendix E.2 instead
        # =====================================================================
        # (Epsilon is computed fresh each iteration via get_epsilon())

        # Record stats
        stats['problem_idx'].append(problem_idx)
        stats['epsilon'].append(epsilon)
        stats['success'].append(success)
        stats['path_cost'].append(path_cost)
        stats['tree_size'].append(tree_size)
        stats['policy_loss'].append(avg_policy_loss)
        stats['value_loss'].append(avg_value_loss)
        stats['total_loss'].append(avg_total_loss)
        stats['num_updates'].append(num_updates_this_problem)

        # Print progress at regular intervals
        if (problem_idx + 1) % args.report_frequency == 0:
            window = min(args.report_frequency, len(recent_successes))
            success_rate = np.mean(recent_successes[-window:])
            avg_path = np.mean(recent_path_costs[-window:]) if recent_path_costs[-window:] else 0

            print(f"\n[Problem {problem_idx + 1}/{num_problems}]")
            print(f"  Epsilon: {epsilon:.2f}")
            print(f"  Success rate (last {window}): {success_rate:.3f}")
            print(f"  Avg path cost (last {window}): {avg_path:.4f}")
            print(f"  Buffer size: {len(replay_buffer)}")
            print(f"  Total gradient steps: {total_gradient_steps}")
            if avg_total_loss >= 0:
                print(f"  Recent losses - Policy: {update_policy_losses[len(update_policy_losses)]:.4f}, Value: {avg_value_loss:.4f}, Total: {avg_total_loss:.4f}")

        # Save checkpoint at milestones
        if (problem_idx + 1) in [1000, 2000] or (problem_idx + 1) % args.save_frequency == 0:
            checkpoint_path = os.path.join(
                args.save_dir,
                f'MSIL_{args.dim}d_problem{problem_idx + 1}.pt'
            )
            torch.save(model.net.state_dict(), checkpoint_path)
            print(f"  Saved checkpoint: {checkpoint_path}")

    # Save final model
    final_path = os.path.join(args.save_dir, f'MSIL_{args.dim}d_final.pt')
    torch.save(model.net.state_dict(), final_path)
    print(f"\nSaved final model: {final_path}")

    # Save training statistics
    stats_path = os.path.join(args.save_dir, f'training_stats_{args.dim}d.npz')
    for key in stats:
        stats[key] = np.array(stats[key])
    np.savez(stats_path, **stats)
    print(f"Saved training stats: {stats_path}")

    # Final summary
    print("\n" + "=" * 70)
    print("Training Complete!")
    print("=" * 70)

    total_successes = sum(stats['success'])
    print(f"\nSummary:")
    print(f"  Total problems processed: {num_problems}")
    print(f"  Overall success rate: {total_successes / num_problems:.3f}")
    print(f"  Total gradient steps: {total_gradient_steps}")
    print(f"  Final buffer size: {len(replay_buffer)}")

    # Success rate by phase
    if num_problems >= 1000:
        phase1_success = np.mean(stats['success'][:1000])
        print(f"  Phase 1 (0-999, eps=1.0) success rate: {phase1_success:.3f}")
    if num_problems >= 2000:
        phase2_success = np.mean(stats['success'][1000:2000])
        print(f"  Phase 2 (1000-1999, eps annealing) success rate: {phase2_success:.3f}")
    if num_problems > 2000:
        phase3_success = np.mean(stats['success'][2000:])
        print(f"  Phase 3 (2000+, eps=0.1) success rate: {phase3_success:.3f}")

    return model, stats


def main():
    parser = argparse.ArgumentParser(
        description='NEXT Meta Self-Improving Learning Training (Algorithm 3)',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Environment settings
    parser.add_argument('--dim', type=int, default=2, choices=[2, 3],
                        help='Dimension of the planning problem (2 or 3)')

    # Training settings matching Algorithm 3
    parser.add_argument('--num-problems', type=int, default=2000,
                        help='Total number of problems to process (N in Algorithm 3)')
    parser.add_argument('--max-samples', type=int, default=500,
                        help='Maximum samples per planning problem (T in Algorithm 1)')
    parser.add_argument('--L', type=int, default=10,
                        help='Number of gradient updates per problem (L in Algorithm 3, lines 6-9)')
    parser.add_argument('--batch-size', type=int, default=8,
                        help='Batch size for sampling from replay buffer')

    # Buffer settings
    parser.add_argument('--buffer-size', type=int, default=2000,
                        help='Capacity of experience replay buffer')
    parser.add_argument('--min-buffer-size', type=int, default=50,
                        help='Minimum buffer size before training starts')

    # Optimizer settings
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate (eta in Algorithm 3)')
    parser.add_argument('--weight-decay', type=float, default=1e-2,
                        help='Weight decay in optimizer')
    parser.add_argument('--lambda-reg', type=float, default=1e-4,
                        help='L2 regularization weight in loss (lambda in Equation 6)')

    # Model settings
    parser.add_argument('--pretrained', type=str, default=None,
                        help='Path to pretrained model weights')

    # System settings
    parser.add_argument('--cuda', action='store_true', default=True,
                        help='Use CUDA if available')
    parser.add_argument('--no-cuda', action='store_false', dest='cuda',
                        help='Disable CUDA')
    parser.add_argument('--seed', type=int, default=1234,
                        help='Random seed')

    # Output settings
    parser.add_argument('--save-dir', type=str, default='trained_models',
                        help='Directory to save model checkpoints')
    parser.add_argument('--save-frequency', type=int, default=500,
                        help='Save checkpoint every N problems')
    parser.add_argument('--report-frequency', type=int, default=100,
                        help='Print progress every N problems')

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("NEXT Meta Self-Improving Learning (MSIL) Training")
    print("Based on Algorithm 3 from Chen et al., ICLR 2020")
    print("=" * 70)
    print("\nConfiguration:")
    print("-" * 40)
    for arg, value in sorted(vars(args).items()):
        print(f"  {arg}: {value}")
    print("-" * 40)

    train(args)


if __name__ == '__main__':
    main()
